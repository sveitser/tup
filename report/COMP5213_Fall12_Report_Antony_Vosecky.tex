\documentclass[12pt,abstracton,a4paper]{scrartcl}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfigure}
\usepackage[margin=2.5cm]{geometry}
%\usepackage[sections=normal,title=normal]{savetrees}
\usepackage[colorlinks]{hyperref}
\usepackage[sorting=none]{biblatex}
\bibliography{bibliography}

% definitions
\def\eqd{\,{\buildrel d \over =}\,}
\def\x{{\bf x}}
\def\z{{\bf z}}
\def\figwidth{0.31\linewidth}
\def\figheigt{3cm}

\title{Topic Models for Twitter User Profiling}
\author{
\begin{tabular}{cc}
    Jan Vosecky & Mathis Antony  \\
    \href{mailto:jvosecky@ust.hk}{jvosecky@ust.hk} &
    \href{mailto:mantony@ust.hk}{mantony@ust.hk}
\end{tabular}
\\ The Hong Kong University of Science and Technology 
\\ Clear Water Bay 
\\ Hong Kong} 
\begin{document}
\maketitle

\begin{abstract}
We propose, implement and analyze two new topic models for twitter user profiling. A challenging aspect of the problem is the evolution of the users and the topics of interests in the general population. We use stochastic variational inference allowing for scalability and treatment of streaming data. We analyze the perplexity of our models output and compare them with the well established Latent Dirichlet Allocation topic model.
\end{abstract}

\section{Introduction}
Latent Dirichlet Allocation (LDA) is a topic model originally invented by Blei et al. \cite{Blei03} as an unsupervised probabilistic graphical model for text analysis. In LDA, every observable word $w$ is supposed to have a hidden topic $k$ and is drawn from a corresponding hidden distribution over the vocabulary $\beta_k$. LDA can be used to firstly discover a set of $k$ hidden topics from a corpus of text documents, and secondly to represent each document as a mixture of the $k$ topics. In general, the exact computation of the posterior in such probabilistic graphical models is intractable. Inference can however be done via approximate methods, such as  \textit{Gibbs-Sampling} \cite{Geman84} or \textit{variational inference} \cite{Bishop06}.

Our goal is to construct a model for Twitter user profiling. The task at hand is similar to topic modeling and we use LDA as a starting point. We also consider the Author-Topic model \cite{Rosen04}, a modification of LDA that captures the relationship between documents and their authors and produces a author topic mixture for each author, based on all documents (co-)authored by the author. However, Twitter has some special properties, which call for modifications of smoothed LDA or the Author-Topic model:
\begin{itemize}
	\item Posts on twitter (referred to as ''tweets``) are very short as they are limited to 140 characters.
	\item Tweets have a single, observable author.
	\item We can expect to have at least a handful of tweets from each author.
	\item Tweets are published one at a time.
	\item In Twitter we need to deal with large-scale data produced in real-time.
\end{itemize}
%
Based on these observations we propose the following two distinct models for Twitter user profiling, shown in Figure \ref{fig:plates}. The main difference between the two models is the Twitter Author Topic Model 1 (TATM1) assumes that each tweet is about a single topic (this is based on the observation of tweets consisting maximally of about two dozen words) whereas in the Twitter Author Topic Model 2 (TATM2) we have a topic assignment on a per word basis. 
%
\begin{figure}
	%\includegraphics[width=0.6\linewidth]{plates}
	\caption{Models in Plate Notation}
	\label{fig:plates}
\end{figure}
%
Both of these models can take advantage of correlations between the tweets of the same author, as the author of a tweet is always known. As our average twitter user is assumed to be alive and tweeting, we are very interested in having an online inference method as opposed to a batch method or standard Gibbs Sampling. An online learning method for LDA, namely \textit{stochastic variational inference} has been developed recently \cite{Hoffman10,Hoffman12}. We use it as a basis to develop an online learning method for our models. The generative process assumed by our models is similar to the author topic model in \cite{Rosen04}. 

\section{Stochastic Variational Inference}
The procedure is derived in detail in \cite{Hoffman12}, here we give a high
level summary of the involved concepts. For this section, we assume $N$ observations ${\bf x}=x_{1:N}$, $N$
local hidden variables ${\bf z}=z_{1:N}$ (in which each $z_n = z_{n,1:J}$ is a
collection of $J$ variables) and a vector of global hidden variables
$\beta$. The joint distribution factors as
\begin{equation}
    p(\beta, z_{1:N}, x_{1:N}) = p(\beta) \prod_{n=1}^N p(z_n,x_n|\beta)
\end{equation}
The distribution of a hidden variable $\xi$ given all other hidden variables and the
observations $\Omega$ is referred to as the complete conditional. The complete conditionals 
of all local and global variables are assumed to be in the exponential family, 
i. e. of the form:
\begin{equation}
    p(\xi|\Omega) =  h_\xi (\xi) \exp \left\{ \eta_\Omega(\Omega)^T t_\xi(\xi) -
        a_\Omega(\eta_\Omega(\Omega))
    \right\} \, .
\end{equation}
where $h_\xi$, $a_\Omega$, $\eta_\Omega$ and $t_\xi$ are called \textit{base
measure}, \textit{log-normalizer}, \textit{natural parameter} and
\textit{sufficient statistics} respectively.

Our goal is to find a distribution $q({\bf z}, \beta)$ which approximates the real posterior as
well as possible. Variational inference minimizes the Kullback-Leibler distance
between the variational distribution $q$ and the true posterior $p({\bf z}, \beta | {\bf
x})$, by maximizing the \textit{evidence lower bound} (ELBO)
\begin{equation}
\mathcal{L}(q) = \mathbb{E}_q[\log p(\x,\z,\beta)] - \mathbb{E}_q[\log q(\z,\beta)]
\end{equation}
which is equal to
the negative of the Kullback-Leibler distance up to an additive constant.

We choose our variational distribution $q$ in the \textit{mean-field family}
\begin{equation}
    q({\bf z}, \beta) = q(\beta | \lambda) \prod_{n=1}^N\prod_{j=1}^J
    q(z_{n,j}|\phi_{n,j}) \,. 
\end{equation}
Now we take $q(\beta|\lambda)$ and $q(z_{n,j}|\phi_{n,j})$ to be in the same exponential family as the complete conditionals which are again in the same family as the prior, i. e.
\begin{eqnarray}
    q(\beta|\lambda) &=& h(\beta) \exp\left\{ \lambda^T t(\beta) -
        a_g(\lambda) \right\} \\
    q(z_{n,j}|\phi_{n,j}) &=& h(z_{n,j}) \exp\left\{ \phi_{n,j}^Tt(z_{n,j})
        -a_{\ell,j}(\phi_{n,j}) \right\} \, .
\end{eqnarray}
The careful choice of distribution makes is rather straightforward to compute both the (Euclidean) gradients as well as the natural gradients which can then be used for coordinate ascent to maximize the ELBO. The (Euclidean) gradients with respect to the global and local variables are respectively
\begin{eqnarray}
    \nabla_\lambda \mathcal L &=& \nabla^2_\lambda a_g(\lambda)\left( \mathbb E_\phi[\eta_g(\x,\z,\alpha)] - \lambda \right) \label{eq:glob-grad} \\
    \nabla_{\phi_{n,j}} \mathcal L &=& \nabla^2_{\phi_{n,j}} a_{\ell,j}({\phi_{n,j}})\left( \mathbb E_{\lambda,\phi_{n,-j}}[\eta_\ell(x_n,z_n,\beta)] - \phi_{n,j} \right) \, . \label{eq:loc:grad}
\end{eqnarray}
It turns out one can obtain an even simpler, yet arguably more useful, expression for the natural gradients. The natural gradient of a function $f$ is given by
\begin{equation}
\hat{\nabla}_\lambda f (\lambda) = G(\lambda)^{-1} \nabla_\lambda f(\lambda) \label{eq:nat-grad}
\end{equation}
where $G(\lambda)$ is the Riemannian metric tensor. If one chooses the symmetrized KL distance 
\begin{equation}
D_\textrm{KL}^{\textrm{sym}}(\lambda,\lambda') =
\mathbb{E}_\lambda \left[ \log \frac{q(\beta|\lambda)}{q(\beta|\lambda')}\right] 
+  \mathbb{E}_\lambda' \left[ \log \frac{q(\beta|\lambda')}{q(\beta|\lambda)}\right]
\end{equation}
which is invariant to parameter transformations as distance measure, the Riemannian metric is the Fisher information matrix
\begin{equation}
    G(\lambda)  = \mathbb{E}_\lambda 
    \left[
    (\nabla_\lambda \log q(\beta|\lambda))
    (\nabla_\lambda \log q(\beta|\lambda))^T
    \right] \,.
\end{equation}
With the choice of $q(\beta|\lambda)$ from the exponential family we have 
\begin{equation}
G(\lambda) = \nabla^2_\lambda a(\lambda).
\end{equation}
Plugging this result together with equation \ref{eq:glob-grad} resp. \ref{eq:loc:grad} into \ref{eq:nat-grad} yields the natural gradients
\begin{eqnarray}
\hat\nabla_\lambda \mathcal L &=& 
 \mathbb E_\phi[\eta_g(\x,\z,\alpha)] - \lambda   \\
\hat\nabla_{\phi_{n,j}} \mathcal L &=& 
 \mathbb E_{\lambda,\phi_{n,-j}}[\eta_\ell(x_n,z_n,\beta)] - \phi_{n,j} \, .
\end{eqnarray}
It turns out in this particular case, with the careful choice of distributions and distance measure the natural gradient is cheaper to compute than the standard (Euclidean) gradient. Another important advantage is that our natural gradient points in the steepest direction in Riemannian space where the distance is measured by the symmetrized KL divergence instead of Euclidean space where the distance measure is simply the Euclidean distance between the two parameter vectors. Gradient ascent with these natural gradients will therefore reduce the dissimilarity between the two distributions more efficiently. For an interesting brief argument for natural gradients and comparison see \cite{Amari98}.


\input{NewModel}
\input{TATM1}
\input{TATM1_Algorithm}
\input{experiments}
\input{conclusion}
\printbibliography
\end{document}
