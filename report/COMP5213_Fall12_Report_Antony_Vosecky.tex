\documentclass[12pt,abstracton,a4paper]{scrartcl}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[colorlinks]{hyperref}
\usepackage[sorting=none]{biblatex}
\bibliography{bibliography.bib}

% definitions
\def\eqd{\,{\buildrel d \over =}\,}
\def\x{{\bf x}}
\def\z{{\bf z}}

\title{Topic Models for Twitter User Profiling}
\author{
\begin{tabular}{cc}
    Jan Vosecky & Mathis Antony  \\
    \href{mailto:jvosecky@ust.hk}{jvosecky@ust.hk} &
    \href{mailto:mantony@ust.hk}{mantony@ust.hk}
\end{tabular}
\\ The Hong Kong University of Science and Technology 
\\ Clear Water Bay 
\\ Hong Kong} 
\begin{document}
\maketitle

\begin{abstract}
We discuss, evaluate and compared various topic models in regards to profiling microblog users according to their interests. A challenging aspect of the problem is the evolution of the users and the topics of interests in the general population. 
\end{abstract}

\section{Introduction}
Latent Dirichlet Allocation (LDA) is a topic model originally invented by Blei et al.\cite{Blei03} as a supervised batch learning model in which every observable word $w$ is supposed to have a hidden topic $k$ and is drawn from a corresponding hidden distribution over the vocabulary $\beta_k$. We use this as a starting point for our models. In general, the exact computation of the posterior in such probabilistic graphical models is intractable. Inference can however be done via approximate methods, such as  \textit{Gibbs-Sampling} \cite{Geman84} or variational inference \cite{Bishop06}.

Our goal is to construct a model for Twitter user profiling. The task at hand is similar to topic modeling but has a few special properties, which call for modifications of smoothed LDA or the author topic model:
\begin{itemize}
	\item Posts on twitter (referred to as ''tweets``) are very short as they are limited to 140 characters.
	\item Tweets have a single, observable author.
	\item Tweets are published one at a time.
\end{itemize}
%
Based on these observation we propose the following two distinct models for twitter user profiling shown in figure \ref{fig:plates}. The main difference between the two models is the Twitter Author Topic Model 1 (TATM1) assumes that each tweet is about a single topic (this is based on the observation of tweets consisting maximally of about two dozen words) whereas in the Twitter Author Topic Model 2 (TATM2) we have a topic assignment on a per word basis. 
%
\begin{figure}
	%\includegraphics[width=0.6\linewidth]{plates}
	\caption{Models in Plate Notation}
	\label{fig:plates}
\end{figure}
%
Both of these models can take advantage of same author correlations as the author of a tweet is always known. As our average twitter user is assumed to be alive and tweeting we are very interested in having an online inference method as opposed to a batch method or standard Gibbs Sampling. An online learning method for LDA, namely \textit{stochastic variational inference} has been developed recently \cite{Hoffman10,Hoffman12}. We use it as a basis to develop an online learning method for our models. The generative process assumed by our models is similar to the author topic model in \cite{Rosen04}. 

\section{Stochastic Variational Inference}
The procedure is derived in detail in \cite{Hoffman12}, here we give a high
level summary of the involved concepts. For this section, we assume $N$ observations ${\bf x}=x_{1:N}$, $N$
local hidden variables ${\bf z}=z_{1:N}$ (in which each $z_n = z_{n,1:J}$ is a
collection of $J$ variables) and a vector of global hidden variables
$\beta$. The joint distribution factors as
\begin{equation}
    p(\beta, z_{1:N}, x_{1:N}) = p(\beta) \prod_{n=1}^N p(z_n,x_n|\beta)
\end{equation}
The distribution of a hidden variable $\xi$ given all other hidden variables and the
observations $\Omega$ is referred to as the complete conditional. The complete conditionals 
of all local and global variable are assumed to be in the exponential family, 
i. e. of the form:
\begin{equation}
    p(\xi|\Omega) =  h_\xi (\xi) \exp \left\{ \eta_\Omega(\Omega)^T t_\xi(\xi) -
        a_\Omega(\eta_\Omega(\Omega))
    \right\} \, .
\end{equation}
where $h_\xi$, $a_\Omega$, $\eta_\Omega$ and $t_\xi$ are called \textit{base
measure}, \textit{log-normalizer}, \textit{natural parameter} and
\textit{sufficient statistics} respectively.

Our goal is to find a distribution $q({\bf z}, \beta)$ which approximates the real posterior as
well as possible. Variational inference minimizes the Kullback-Leibler distance
between the variational distribution $q$ and the true posterior $p({\bf z}, \beta | {\bf
x})$, by maximizing the \textit{evidence lower bound} (ELBO)
\begin{equation}
\mathcal{L}(q) = \mathbb{E}_q[\log p(\x,\z,\beta)] - \mathbb{E}_q[\log q(\z,\beta)]
\end{equation}
which is equal to
the negative of the Kullback-Leibler distance up to an additive constant.

We choose our variational distribution $q$ in the \textit{mean-field family}
\begin{equation}
    q({\bf z}, \beta) = q(\beta | \lambda) \prod_{n=1}^N\prod_{j=1}^J
    q(z_{n,j}|\phi_{n,j}) \,. 
\end{equation}
Now we take $q(\beta|\lambda)$ and $q(z_{n,j}|\phi_{n,j})$ to be in the same exponential family as the complete conditionals which are again in the same family as the prior, i. e.
\begin{eqnarray}
    q(\beta|\lambda) &=& h(\beta) \exp\left\{ \lambda^T t(\beta) -
        a_g(\lambda) \right\} \\
    q(z_{n,j}|\phi_{n,j}) &=& h(z_{n,j}) \exp\left\{ \phi_{n,j}^Tt(z_{n,j})
        -a_{\ell,j}(\phi_{n,j}) \right\} \, .
\end{eqnarray}
The careful choice of distribution makes is rather straightforward to compute both the (Euclidean) gradients as well as the natural gradients which can then be used for coordinate ascent to maximize the ELBO. The (Euclidean) gradients with respect to the global and local variables are respectively
\begin{eqnarray}
    \nabla_\lambda \mathcal L &=& \nabla^2_\lambda a_g(\lambda)\left( \mathbb E_\phi[\eta_g(\x,\z,\alpha)] - \lambda \right) \label{eq:glob-grad} \\
    \nabla_{\phi_{n,j}} \mathcal L &=& \nabla^2_{\phi_{n,j}} a_{\ell,j}({\phi_{n,j}})\left( \mathbb E_{\lambda,\phi_{n,-j}}[\eta_\ell(x_n,z_n,\beta)] - \phi_{n,j} \right) \, . \label{eq:loc:grad}
\end{eqnarray}
It turns out one can obtain an even simpler, yet arguably more useful, expression for the natural gradients. The natural gradient of a function $f$ is given by
\begin{equation}
\hat{\nabla}_\lambda f (\lambda) = G(\lambda)^{-1} \nabla_\lambda f(\lambda) \label{eq:nat-grad}
\end{equation}
where $G(\lambda)$ is the Riemannian metric tensor. If one chooses the symmetrized KL distance 
\begin{equation}
D_\textrm{KL}^{\textrm{sym}}(\lambda,\lambda') =
\mathbb{E}_\lambda \left[ \log \frac{q(\beta|\lambda)}{q(\beta|\lambda')}\right] 
+  \mathbb{E}_\lambda' \left[ \log \frac{q(\beta|\lambda')}{q(\beta|\lambda)}\right]
\end{equation}
which is invariant to parameter transformations as distance measure, the Riemannian metric is the Fisher information matrix
\begin{equation}
    G(\lambda)  = \mathbb{E}_\lambda 
    \left[
    (\nabla_\lambda \log q(\beta|\lambda))
    (\nabla_\lambda \log q(\beta|\lambda))^T
    \right] \,.
\end{equation}
With the choice of $q(\beta|\lambda)$ from the exponential family we have 
\begin{equation}
G(\lambda) = \nabla^2_\lambda a(\lambda).
\end{equation}
Plugging this result together with equation \ref{eq:glob-grad} resp. \ref{eq:loc:grad} into \ref{eq:nat-grad} yields the natural gradients
\begin{eqnarray}
\hat\nabla_\lambda \mathcal L &=& 
 \mathbb E_\phi[\eta_g(\x,\z,\alpha)] - \lambda   \\
\hat\nabla_{\phi_{n,j}} \mathcal L &=& 
 \mathbb E_{\lambda,\phi_{n,-j}}[\eta_\ell(x_n,z_n,\beta)] - \phi_{n,j} \, .
\end{eqnarray}



\input{NewModel}

\input{TATM1_Algorithm}

\printbibliography
\end{document}
